{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nfrom datasets import load_dataset\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nimport os\n\nclass BengaliDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length_source=128, max_length_target=64):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length_source = max_length_source\n        self.max_length_target = max_length_target\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def clean_text(self, text: str) -> str:\n        \"\"\"Clean and normalize text\"\"\"\n        text = text.strip()\n        text = ' '.join(text.split())\n        return text\n    \n    def __getitem__(self, idx):\n        # Get and clean Banglish and Bangla text\n        banglish = self.clean_text(str(self.data[idx]['rm']))\n        bangla = self.clean_text(str(self.data[idx]['bn']))\n        \n        # Tokenize Banglish text\n        inputs = self.tokenizer(\n            banglish,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length_source,\n            return_tensors=\"pt\"\n        )\n        \n        # Tokenize Bangla text\n        with self.tokenizer.as_target_tokenizer():\n            labels = self.tokenizer(\n                bangla,\n                padding='max_length',\n                truncation=True,\n                max_length=self.max_length_target,\n                return_tensors=\"pt\"\n            )\n        \n        return {\n            'input_ids': inputs.input_ids.squeeze(),\n            'attention_mask': inputs.attention_mask.squeeze(),\n            'labels': labels.input_ids.squeeze()\n        }\n\ndef train_model(model, train_dataloader, val_dataloader, \n                device='cuda',\n                num_epochs=5,\n                learning_rate=2e-5,\n                warmup_steps=500,\n                gradient_accumulation_steps=4,\n                max_grad_norm=1.0,\n                save_path='./best_model'):\n    \"\"\"Train the model with explicit saving and progress tracking\"\"\"\n    \n    print(f'\\nTraining Configuration:')\n    print(f'Learning Rate: {learning_rate}')\n    print(f'Number of Epochs: {num_epochs}')\n    print(f'Warmup Steps: {warmup_steps}')\n    print(f'Save Path: {save_path}')\n    print('-' * 50)\n    \n    # Set up optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n    \n    # Set up scheduler\n    num_training_steps = len(train_dataloader) * num_epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=num_training_steps\n    )\n    \n    # Training loop\n    best_val_loss = float('inf')\n    model_save_successful = False\n    \n    for epoch in range(num_epochs):\n        print(f'\\nStarting Epoch {epoch + 1}/{num_epochs}')\n        \n        # Training phase\n        model.train()\n        total_loss = 0\n        \n        progress_bar = tqdm(train_dataloader, desc=f'Training Epoch {epoch+1}')\n        for step, batch in enumerate(progress_bar):\n            # Move batch to device\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            # Forward pass\n            outputs = model(**batch)\n            loss = outputs.loss / gradient_accumulation_steps\n            \n            # Backward pass\n            loss.backward()\n            \n            if (step + 1) % gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n            \n            total_loss += loss.item() * gradient_accumulation_steps\n            progress_bar.set_postfix({'loss': loss.item() * gradient_accumulation_steps})\n        \n        avg_loss = total_loss / len(train_dataloader)\n        print(f'Epoch {epoch+1} - Average training loss: {avg_loss:.4f}')\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0\n        print('\\nStarting validation...')\n        \n        with torch.no_grad():\n            for batch in tqdm(val_dataloader, desc='Validating'):\n                batch = {k: v.to(device) for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss += outputs.loss.item()\n        \n        avg_val_loss = val_loss / len(val_dataloader)\n        print(f'Epoch {epoch+1} - Validation loss: {avg_val_loss:.4f}')\n        \n        # Save best model\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            try:\n                print(f'\\nSaving model to {save_path}...')\n                os.makedirs(save_path, exist_ok=True)\n                model.save_pretrained(save_path)\n                print(f'Model successfully saved to {save_path}')\n                model_save_successful = True\n            except Exception as e:\n                print(f'Error saving model: {str(e)}')\n                model_save_successful = False\n    \n    if not model_save_successful:\n        print('\\nWARNING: Model was not saved successfully during training!')\n    \n    return model, model_save_successful\n\ndef test_translations(model, tokenizer, test_texts, device):\n    \"\"\"Test the model with multiple translations\"\"\"\n    print('\\nStarting translation tests...')\n    model.eval()\n    results = []\n    \n    for test_text in test_texts:\n        try:\n            print(f'\\nTranslating: {test_text}')\n            inputs = tokenizer(test_text, return_tensors=\"pt\").to(device)\n            \n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    forced_bos_token_id=tokenizer.lang_code_to_id[\"bn_IN\"],\n                    num_beams=5,\n                    max_length=64,\n                    early_stopping=True\n                )\n            \n            bangla_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n            results.append({\n                'input': test_text,\n                'output': bangla_text\n            })\n            print(f'Banglish: {test_text}')\n            print(f'Bangla: {bangla_text}')\n            print('-' * 50)\n            \n        except Exception as e:\n            print(f'Error translating \"{test_text}\": {str(e)}')\n    \n    return results\n\ndef main():\n    # Set device\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f'Using device: {device}')\n    \n    # Initialize tokenizer and model\n    print('\\nLoading tokenizer and model...')\n    model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n    tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n    model = MBartForConditionalGeneration.from_pretrained(model_name)\n    \n    # Set source and target languages\n    tokenizer.src_lang = \"en_XX\"\n    tokenizer.tgt_lang = \"bn_IN\"\n    \n    # Move model to device\n    model.to(device)\n    \n    # Load and prepare data\n    print('\\nLoading dataset...')\n    dataset = load_dataset(\"SKNahin/bengali-transliteration-data\")\n    \n    print('\\nPreparing data splits...')\n    train_test = dataset['train'].train_test_split(test_size=0.2, shuffle=True, seed=42)\n    train_val = train_test['train'].train_test_split(test_size=0.1, shuffle=True, seed=42)\n    \n    print('Creating datasets and dataloaders...')\n    train_dataset = BengaliDataset(train_val['train'], tokenizer)\n    val_dataset = BengaliDataset(train_val['test'], tokenizer)\n    \n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=8,\n        shuffle=True,\n        num_workers=4,\n        pin_memory=True\n    )\n    \n    val_dataloader = DataLoader(\n        val_dataset,\n        batch_size=16,\n        num_workers=2,\n        pin_memory=True\n    )\n    \n    # Train model\n    print('\\nStarting training process...')\n    save_path = './best_model'\n    model, save_successful = train_model(\n        model,\n        train_dataloader,\n        val_dataloader,\n        device=device,\n        num_epochs=10,\n        learning_rate=2e-5,\n        warmup_steps=500,\n        save_path=save_path\n    )\n    \n    if not save_successful:\n        print('\\nERROR: Model was not saved properly. Please check disk space and permissions.')\n        return\n    \n    # Test translations\n    test_texts = [\n        \"bangla amar matribhasha\",\n        \"ami tomake bhalobashi\",\n        \"kemon acho bondhu\"\n    ]\n    \n    results = test_translations(model, tokenizer, test_texts, device)\n    \n    # Final summary\n    print('\\nProcess Summary:')\n    print(f'Model saved at: {save_path}')\n    print(f'Number of test translations completed: {len(results)}')\n    print('\\nProcess completed!')\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T13:36:57.692070Z","iopub.execute_input":"2025-01-03T13:36:57.692365Z","iopub.status.idle":"2025-01-03T14:13:10.653901Z","shell.execute_reply.started":"2025-01-03T13:36:57.692344Z","shell.execute_reply":"2025-01-03T14:13:10.653016Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nLoading tokenizer and model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cfcf1d5b9fe4ba9a128a70aa3d5a5f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8120881889847fbaccdba42f3e8b033"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04f6c7c519394b10ae7bd174f37ceca7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"379199c30e404207be585cc3a7465dfa"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37e6839bb8124277b00f5d0dc0ac5dd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38f88bc0813e4b18ae041465959245b6"}},"metadata":{}},{"name":"stdout","text":"\nLoading dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/300 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f33c50268d442b08237e8b0493420bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/333k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"428001832a7c4ab79ca385ce2158be6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/5006 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c6815c539b44fee8fc00ded2d9500c5"}},"metadata":{}},{"name":"stdout","text":"\nPreparing data splits...\nCreating datasets and dataloaders...\n\nStarting training process...\n\nTraining Configuration:\nLearning Rate: 2e-05\nNumber of Epochs: 10\nWarmup Steps: 500\nSave Path: ./best_model\n--------------------------------------------------\n\nStarting Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:   0%|          | 0/451 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nTraining Epoch 1: 100%|██████████| 451/451 [03:19<00:00,  2.26it/s, loss=8.38]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Average training loss: 10.0374\n\nStarting validation...\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nValidating: 100%|██████████| 26/26 [00:06<00:00,  3.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Validation loss: 8.2998\n\nSaving model to ./best_model...\n","output_type":"stream"},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"Model successfully saved to ./best_model\n\nStarting Epoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2:   0%|          | 0/451 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nTraining Epoch 2: 100%|██████████| 451/451 [03:19<00:00,  2.26it/s, loss=3.03]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Average training loss: 6.0531\n\nStarting validation...\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nValidating: 100%|██████████| 26/26 [00:06<00:00,  3.88it/s]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Validation loss: 2.9149\n\nSaving model to ./best_model...\nModel successfully saved to ./best_model\n\nStarting Epoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3:   0%|          | 0/451 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nTraining Epoch 3: 100%|██████████| 451/451 [03:19<00:00,  2.26it/s, loss=0.383]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Average training loss: 1.1175\n\nStarting validation...\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nValidating: 100%|██████████| 26/26 [00:06<00:00,  3.87it/s]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Validation loss: 0.3479\n\nSaving model to ./best_model...\nModel successfully saved to ./best_model\n\nStarting Epoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4:   0%|          | 0/451 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nTraining Epoch 4: 100%|██████████| 451/451 [03:19<00:00,  2.26it/s, loss=0.0555]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Average training loss: 0.2604\n\nStarting validation...\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nValidating: 100%|██████████| 26/26 [00:06<00:00,  3.88it/s]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Validation loss: 0.2254\n\nSaving model to ./best_model...\nModel successfully saved to ./best_model\n\nStarting Epoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5:   0%|          | 0/451 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nTraining Epoch 5: 100%|██████████| 451/451 [03:19<00:00,  2.26it/s, loss=0.113] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Average training loss: 0.1454\n\nStarting validation...\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nValidating: 100%|██████████| 26/26 [00:06<00:00,  3.88it/s]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Validation loss: 0.1885\n\nSaving model to ./best_model...\nModel successfully saved to ./best_model\n\nStarting Epoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 6:   0%|          | 0/451 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nTraining Epoch 6: 100%|██████████| 451/451 [03:19<00:00,  2.26it/s, loss=0.315] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 - Average training loss: 0.0919\n\nStarting validation...\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nValidating: 100%|██████████| 26/26 [00:06<00:00,  3.88it/s]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 - Validation loss: 0.1735\n\nSaving model to ./best_model...\nModel successfully saved to ./best_model\n\nStarting Epoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 7:   0%|          | 0/451 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nTraining Epoch 7: 100%|██████████| 451/451 [03:19<00:00,  2.26it/s, loss=0.0917]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 - Average training loss: 0.0617\n\nStarting validation...\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nValidating: 100%|██████████| 26/26 [00:06<00:00,  3.88it/s]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 - Validation loss: 0.1643\n\nSaving model to ./best_model...\nModel successfully saved to ./best_model\n\nStarting Epoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 8:   0%|          | 0/451 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nTraining Epoch 8: 100%|██████████| 451/451 [03:19<00:00,  2.26it/s, loss=0.0725] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 - Average training loss: 0.0453\n\nStarting validation...\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nValidating: 100%|██████████| 26/26 [00:06<00:00,  3.87it/s]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 - Validation loss: 0.1582\n\nSaving model to ./best_model...\nModel successfully saved to ./best_model\n\nStarting Epoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 9:   0%|          | 0/451 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nTraining Epoch 9: 100%|██████████| 451/451 [03:19<00:00,  2.26it/s, loss=0.00563]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 - Average training loss: 0.0345\n\nStarting validation...\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nValidating: 100%|██████████| 26/26 [00:06<00:00,  3.88it/s]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 - Validation loss: 0.1578\n\nSaving model to ./best_model...\nModel successfully saved to ./best_model\n\nStarting Epoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 10:   0%|          | 0/451 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nTraining Epoch 10: 100%|██████████| 451/451 [03:19<00:00,  2.26it/s, loss=0.0414] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 - Average training loss: 0.0273\n\nStarting validation...\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nValidating: 100%|██████████| 26/26 [00:06<00:00,  3.88it/s]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 - Validation loss: 0.1531\n\nSaving model to ./best_model...\nModel successfully saved to ./best_model\n\nStarting translation tests...\n\nTranslating: bangla amar matribhasha\nBanglish: bangla amar matribhasha\nBangla: বাংলা আমার মাধ্যমে\n--------------------------------------------------\n\nTranslating: ami tomake bhalobashi\nBanglish: ami tomake bhalobashi\nBangla: আমি তোমাকে ভালোবাসি\n--------------------------------------------------\n\nTranslating: kemon acho bondhu\nBanglish: kemon acho bondhu\nBangla: কেমন এত বন্ধু\n--------------------------------------------------\n\nProcess Summary:\nModel saved at: ./best_model\nNumber of test translations completed: 3\n\nProcess completed!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}